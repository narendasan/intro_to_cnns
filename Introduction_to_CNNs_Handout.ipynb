{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to CNNs Handout.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/narendasan/intro_to_cnns/blob/master/Introduction_to_CNNs_Handout.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "wYfACiqaj782",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifying Handwritting using Convolutional Neural Networks\n",
        "\n",
        "In this example we are going to use PyTorch, a commonly used Deep Learning Framework to classify Handwritten Digits.\n",
        "\n",
        "We are going to motivate the use of CNNs by quickly examining Softmax Regression and Multilayer Perceptrons, then use the LeNet5 Network Architecture, an architecture developed by Yann LeCunn in the 1990s to recognize digits on zipcodes."
      ]
    },
    {
      "metadata": {
        "id": "DF6NyMrekPIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "PyTorch is a python based framework and does much of the heavy lifting for us when it comes to preprocessing our data and managing the learning process.\n",
        "\n",
        "We are going to start by installing PyTorch, importing the framework package and the other packages we are going to use\n"
      ]
    },
    {
      "metadata": {
        "id": "_8IKYx0bj86k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-linux_x86_64.whl\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
        "!pip3 install numpy\n",
        "!pip3 install matplotlib\n",
        "!pip3 install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBbDApmHDS5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading Helpful Packages\n",
        "First thing in any app is to include all of the dependencies for the project.\n",
        "\n",
        "\n",
        "\n",
        "1.   torch (PyTorch) and its specific sub-libraries - This handles all of our neural network setup, training and testing \n",
        "2.   torchvision - This is a specific sub-library for PyTorch for computer vision tasks\n",
        "3.   numpy - Generic matrix library for python\n",
        "4.   matplotlib and seaborn - graphing libraries for python\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "W3QR77nRkR4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable \n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcfM99agEFHO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up the Enviorment\n",
        "We are training our networks on GPUs so we need to make sure to see the random number generator there before we begin"
      ]
    },
    {
      "metadata": {
        "id": "72V5qql9lLpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Enable Cuda\n",
        "SEED = 1                              #Seed the random wieghts on initialization\n",
        "LOG_INTERVAL = 100                    #How often to log training and testing info \n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkvzfupcEOw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading our Data\n",
        "First thing we are going to do is load our dataset. "
      ]
    },
    {
      "metadata": {
        "id": "wz9hy-CxaQjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This dataset is called MNIST and it was created by Yann Lecun as a benchmark for handwritting recognition. \n",
        "As recognition systems got better, it transitioned to a way to teach people the fundimentals of image classification. The dataset consists of 60,000 training and 10,000 testing 28x28px images from 0-9 in grayscale and their labels. Because it is a common example PyTorch makes it easy to download and setup. \n",
        "\n",
        "\n",
        "We want to seperate our training and testing data to prevent what is called **overfitting**. Overfitting is when the model starts to memorize what is in its training set and when that happens it will not be as flexible to new images. So we never want to evaluate the performace of the model on the training data. We use a set of data the network has not seen before to assess how good the network is.\n",
        "\n",
        "\n",
        "### Dataset Hyper Parameters\n",
        "\n",
        "Notice there are two \"**Hyper Parameters**\", numbers that are not optimized by the model but have impact on the performace. There are more hyper parameters below but since we are right now creating our dataset we are going to start with these two: **Batch Size** and **Test Batch Size**. Batch Size defines how many images the model will try to classify before trying to update its weights. Since we are using **Stocastic Gradient Decent** (randomly sampling datapoints from the dataset instead of looking at the set in order), we can get irratic gradient behavior if we update too often with not enough information. A lot of times increasing batch size will improve accuracy of the network and the speed of training (by allowing you to take larger steps), but there is a hard limit on the size of your steps so ever increasing batch size may not be a good idea. The test batch size is less important and only defines how many instances of the model we evaluate at once.\n",
        "\n",
        "\n",
        "\n",
        "Feel free to tweak these numbers and see how it effects the training and accuracy of your models. Make sure to ```shift + enter``` after setting them to lock the values in\n",
        "\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "We dont usually just pass raw images to our model, since we want to reduce the varience in our dataset so distingushing images is an easier task. There are a couple common preprocessing techniques such as subtracting the mean of the dataset from each image and normalizing images. \n",
        "\n",
        "\n",
        "\n",
        "![Data Preprocessing - Credit: CS231n Stanford](http://cs231n.github.io/assets/nn2/prepro1.jpeg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We are going to normalize our data. Luckly pytorch makes this easy, using the transforms lambda functions. I provided the normalization values for you."
      ]
    },
    {
      "metadata": {
        "id": "u3m46zeMlNDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Dataloader\n",
        "#@title Batch Hyper Parameters\n",
        "BATCH_SIZE = 64           #@param {type:\"integer\"}  #Number of training images to process before updating weights\n",
        "TEST_BATCH_SIZE = 1000    #@param {type:\"number\"}   #Number of test images to process at once\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "train_loader  = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('/tmp/mnist/data', \n",
        "                   train=True, \n",
        "                   download=True, \n",
        "                   transform=transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,),\n",
        "                                                     (0.3081,))\n",
        "                              ])),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('/tmp/mnist/data', \n",
        "                   train=False, \n",
        "                   transform=transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,),\n",
        "                                                     (0.3081,))\n",
        "                                ])),\n",
        "    batch_size=TEST_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    **kwargs\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfRjTshDiK5Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the Learning Pipeline \n",
        "\n",
        "We are going now to layout all the systems to manage training and testing our model"
      ]
    },
    {
      "metadata": {
        "id": "zKMn-92HHuB8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Training Pipeline\n",
        "First we are going to define our **training pipeline**. This is the order of steps in which you go though to train your model on how to complete its task. \n",
        "First we start with setting the model to train mode. This allows the parameters in the model to change. This function represents 1 training **epoch**. An epoch is one full interation through the data set. We use the training dataset we created earlier to generate batches of data and their associated true labels. \n",
        "\n",
        "For each batch we go through the following steps:\n",
        "\n",
        "1. Move our data to our GPU\n",
        "2. Declare our data as Variables in the network\n",
        "3. Zero all the gradients in the network\n",
        "3. Run our data through the network and get the predictions\n",
        "4. Using out chosen loss function, calculate the loss function\n",
        "5. Calcuate the gradients $\\nabla_W$ of the loss with respect to the weights at each node in the network \n",
        "5. Update the weights in the network using $W_{t+1} = W_t - \\alpha \\nabla_W\\mathcal L$"
      ]
    },
    {
      "metadata": {
        "id": "Y3I4OdWz2M-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_func, epoch, training_history):\n",
        "    model.train()                                            #Set training mode \n",
        "    for batch, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.cuda(), target.cuda()            #Transfer data and correct result to the GPU\n",
        "        data, target = Variable(data), Variable(target)      #Make data and correct answers variables in the Network\n",
        "        optimizer.zero_grad()                                #zero the gradients\n",
        "        output = model(data)                                 #classify the data\n",
        "        loss = loss_func(output, target)                     #calculate the loss\n",
        "        loss.backward()                                      #propogate the weight updates through the network\n",
        "        optimizer.step()\n",
        "        if batch % LOG_INTERVAL == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch * len(data), len(train_loader.dataset), 100. * batch / len(train_loader), loss.data[0]))\n",
        "            training_history.append(((len(train_loader.dataset) * epoch) + batch * len(data), loss.data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKpltRSSKTkU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing Pipeline\n",
        "Next we are going to define our **testing or inference pipeline**. During our training we want to intermittently chech the accuracy of our model. Since we don't want to do this with data the model has already seen we will show it new data that it will not remember since we will not be doing backpropogation here. You can see that the pipeline is almost the same with the changes after the data passes through the model and loss is calculated. Here we look at the probablilites outputed by the final layer of the models and find the index of the one with the highest. Each node corressponds to its index's value (e.g. index 0 means 0). We then compare the index of what was predicted to what was expected and calcuated the accuracy."
      ]
    },
    {
      "metadata": {
        "id": "JO0hgRhT3tEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, loss_func, epoch, test_loss_history, test_accuracy_history):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.cuda(), target.cuda()                      #Transfer data and correct result to the GPU\n",
        "        data, target = Variable(data, volatile=True), Variable(target) #Make data and correct answers variables in the Network\n",
        "        output = model(data)                                           #classify the data\n",
        "        test_loss += loss_func(output, target).data[0]                 #calculate loss\n",
        "        pred = output.data.max(1)[1]                                   #get predictions for the batch using argmax\n",
        "        correct += pred.eq(target.data).cpu().sum()                    #total correct anwsers \n",
        "    test_loss /= len(test_loader)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    test_loss_history.append((epoch, test_loss))\n",
        "    test_accuracy_history.append((epoch, 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRuITEFVh5Ns",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function controls our full training of the model. It takes in a model definition, a selected optimizer and selected loss function and then returns metrics on how the training went and as a byproduct also trains the model. As you can see the learning pipeline iterates according to the number of epochs or the number of times that you want the model to go though the full dataset"
      ]
    },
    {
      "metadata": {
        "id": "71n4Eepdhz5U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def learn(model, optimizer, loss_func):\n",
        "    training_loss = []\n",
        "    test_loss = []\n",
        "    test_accuracy = []\n",
        "    for e in range(EPOCHS):\n",
        "        train(model, optimizer, loss_func, e + 1, training_loss)\n",
        "        test(model, loss_func, e + 1, test_loss, test_accuracy)\n",
        "    return (training_loss, test_loss, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZHBC9u5mLU2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helpful Utilities\n",
        "The following function is a function to visualize the learning process, plotting the training loss, testing loss and testing accuracy over time. "
      ]
    },
    {
      "metadata": {
        "id": "siakJmVD32B2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_learning(training_loss, test_loss, test_accuracy):\n",
        "    f1 = plt.figure()\n",
        "    f2 = plt.figure()\n",
        "    f3 = plt.figure()\n",
        "    ax1 = f1.add_subplot(111)\n",
        "    ax2 = f2.add_subplot(111)\n",
        "    ax3 = f3.add_subplot(111)\n",
        "\n",
        "    training_loss_batch, training_loss_values = zip(*training_loss)\n",
        "    ax1.plot(training_loss_batch, training_loss_values)\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel(\"Batch\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "\n",
        "    test_loss_epoch, test_loss_values = zip(*test_loss)\n",
        "    ax2.plot(test_loss_epoch, test_loss_values)\n",
        "    ax2.set_title(\"Testing Loss\")\n",
        "    ax2.set_xlabel(\"Batch\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "\n",
        "    test_accuracy_epoch, test_accuracy_values = zip(*test_accuracy)\n",
        "    ax3.plot(test_accuracy_epoch, test_accuracy_values)\n",
        "    ax3.set_title(\"Testing Accuracy\")\n",
        "    ax3.set_xlabel(\"Batch\")\n",
        "    ax3.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6mf8ihXME1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function conducts inference on a single image"
      ]
    },
    {
      "metadata": {
        "id": "6Rb_j3Mc_qxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classify(model, img):\n",
        "    img = img.cuda()\n",
        "    img = Variable(img, volatile=True)\n",
        "    output = model(img)\n",
        "    return output.data.max(1)[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2adnU7tMMIaa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function takes an image from the testing set and runs inference on it"
      ]
    },
    {
      "metadata": {
        "id": "WLo7idJB_vRm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classify_an_example(model):\n",
        "    img = next(iter(test_loader))[0]\n",
        "    img_np = img.cpu().numpy()[0]\n",
        "    plt.imshow(img_np.reshape(28,28))\n",
        "    print()\n",
        "    print(\"The image is probably: {}\".format(classify(model, img)[0]))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfHivH42ujUc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing Sofmax Regression"
      ]
    },
    {
      "metadata": {
        "id": "dWsgeSsbMOmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our first model of the day is going to be softmax regression. This model can be summed up as simultaneously calculating the probablity that an image is any of the 10 classes at the same time using a set of parameters that represent the importance of particular pixels in the image. \n",
        "\n",
        "We use the function $P(y' = \\{1..10\\}) = \\sigma(w^Tx + b)$ where $\\sigma = \\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i}}$ to calculate the probablity and we use gradient decent to find the weights. \n"
      ]
    },
    {
      "metadata": {
        "id": "1Rs0kBMjinJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the Model\n",
        "Pytorch has a super easy way to define machine learning models. We create a sub class of the pytorch class ```torch.nn.Module``` listing out the layers of our model in our ```__init__``` method and the operations between those layers in the method called ```forward```. The ```torch.nn.Module``` super class handles the implementation of the backpropogation.\n",
        "\n",
        "Go a head and try to fill in what you think a SoftmaxRegression Model would look like.\n",
        "\n",
        "Here are some useful functions:\n",
        "\n",
        "- ```torch.nn.Linear``` -> Fully Connected Layer / Impliments $F(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + \\boldsymbol b$\n",
        "- ```torch.nn.Functional.softmax``` -> Softmax function / Implements $F(x) = \\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} }$\n",
        "- ```torch.nn.Functional.log_softmax``` -> Log of the softmax function / Implements $F(x) = log(\\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} })$\n",
        "- ```torch.Tensor.view``` -> allows you to reshape a **Tensor** (multi-dimentional vector)"
      ]
    },
    {
      "metadata": {
        "id": "_vf5mWsxupFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        '''\n",
        "        INSERT YOUR MODEL COMPONENTS HERE\n",
        "        ''' \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        LINK YOUR COMPONENTS HERE \n",
        "        '''\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbrDKZg6R2lu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "Now that we have a model definition, we can train it on our dataset. Below you can see a couple more hyper parameters. \n",
        "\n",
        "- **epochs** -> The number of times we show the training dataset to the model \n",
        "- **learning rate** -> The size of the step we take each time we go through backpropogation (i.e. the $\\alpha$ in $W_{t+1} = W_t - \\alpha \\nabla_W\\mathcal L$)\n",
        "- **momentum** (_Note: this will only apply if you use SGD_) -> We don't want to get stuck in local minima in the loss landscape so we may want to not let a particularly bad batch prevent otherwise good progress. We can redefine the parameter update procedure as $W_{t+1} = W_t - \\alpha V_t$ and $V_t = \\beta V_{t-1} + (1 - \\beta)\\nabla_W\\mathcal L$ where $\\beta$\n",
        "is our notion of momentum\n",
        "\n",
        "Set these values to something you think might be reasonable and see what happens. Make sure to ```shift+enter``` to lock them in."
      ]
    },
    {
      "metadata": {
        "id": "RlmXFR7UBUo4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Training Hyper Parameters\n",
        "EPOCHS = 10                #@param {type:\"integer\"}           #Number of times to go through the data set\n",
        "LEARNING_RATE = 0.001     #@param {type:\"number\"}            #How far each update pushes the weights\n",
        "SGD_MOMENTUM = 0.5        #@param {type:\"number\"}            #How much it takes to change the direction of the gradient\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIWaEZBhcrVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can create an instance of our model and transfer it to the GPU (using the ```nn.Module.cuda``` method).\n",
        "\n",
        "Now chose your optimizer, there are a bunch of choices (the most common one is **Stocastic Gradient Decent**, but others include ADAM, ADA and more). Take a look at http://pytorch.org/docs/master/optim.html#algorithms for a list of choices, but if you need a recomendation use ```torch.optim.SGD```\n",
        "\n",
        "Next we need to pick a **loss** function. This function calcaluates how far off our prediction was from the expected value. Again there are a whole bunch of options. Here is a list http://pytorch.org/docs/master/nn.html#id46\n",
        "If you are not sure what to choose then use ```torch.nn.functional.nll_loss``` which is negative log likelihood loss ($L(y) = -log(y)$) a common function to use with softmax\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MOHTgx03BdT8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sr_model = SoftmaxRegression()\n",
        "sr_model.cuda()\n",
        "# Change this to whatever optimizer you want to try\n",
        "sr_optimizer = optim.SGD(sr_model.parameters(), lr=LEARNING_RATE, momentum=SGD_MOMENTUM)\n",
        "print(sr_model)\n",
        "#Change the last argument to whatever loss function you want to try \n",
        "SR_TRAINING_LOSS, SR_TEST_LOSS, SR_TEST_ACCURACY = learn(sr_model, sr_optimizer, F.nll_loss) #negative loss values may be due to your final layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INxKbaMXi7X-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing How Well the Training Went\n",
        "\n",
        "Now we should have a reasonably well trained model. Lets see how the train went. Use the visualization function to plot the **training loss**, **testing loss** and **testing accuracy** over the course of the training. \n",
        "\n",
        "_Notice: We do not measure the training accuracy, since it is not a good measure of how good the model is since we are looking for a model that has good performace **on data not yet seen**_\n",
        "\n",
        "We should see the training loss quickly decline then plateau. \n",
        "  - If you are seeing jagged training loss perhaps increase the batch size or decrease the learning rate \n",
        "  \n",
        "You should see that the testing loss more gradually reduces \n",
        "  - if you see the testing loss starting to go up, you are now overfitting - probably reduce the number of epochs\n",
        "\n",
        "You should also see the testing accuracy increase at the same rate the testing loss decreases"
      ]
    },
    {
      "metadata": {
        "id": "oXV30DFXBwvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_learning(SR_TRAINING_LOSS, SR_TEST_LOSS, SR_TEST_ACCURACY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dly0VRlPljhK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trying an Example on Your Trained Model\n",
        "\n",
        "Use the ```classify_an_example``` helper function to classify an example from the testing set"
      ]
    },
    {
      "metadata": {
        "id": "HjbEmbtOBzxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classify_an_example(sr_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0sZxJW0OzAMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing Multilayer Perceptron\n",
        "\n",
        "Next we are going to implement a mulilayer perceptron. Each layer weights the importance of the data from the previous layer, eventually terminating in the same sigmoid function to convert the representation vector of the input data into a probablity. Between each layer we now include a **non-linearity or activation function**, a function that allows the model to approximate non linear functions and filter data between layers.\n",
        "\n",
        "Each layer of the MLP now looks like this $relu(w^Tx +b)$ and the full model becomes $\\sigma(w^T relu(w^T relu(w^Tx + b)+ b) + b)$\n",
        "\n",
        "Again we train this model with gradient decent"
      ]
    },
    {
      "metadata": {
        "id": "-p_YFjAKnTD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the Model\n",
        "Once again we are going to create a sub class of the pytorch class ```torch.nn.Module``` listing out the layers of our model in our ```__init__``` method and the operations between those layers in the method called ```forward```. The ```torch.nn.Module``` super class handles the implementation of the backpropogation.\n",
        "\n",
        "Go a head and try to fill in what you think a Multilayer Perceptron Model would look like.\n",
        "\n",
        "Here are some useful functions:\n",
        "\n",
        "- ```torch.nn.Linear``` -> Fully Connected Layer / Impliments $F(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + \\boldsymbol b$\n",
        "- ```torch.nn.Functional.relu``` -> Rectifying Linear Unit / Implements $max(0,x)$ \n",
        "    - Other activation functions can be found here: http://pytorch.org/docs/master/nn.html#non-linear-activation-functions \n",
        "- ```torch.nn.Functional.softmax``` -> Softmax function / Implements $F(x) = \\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} }$\n",
        "- ```torch.nn.Functional.log_softmax``` -> Log of the softmax function / Implements $F(x) = log(\\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} })$\n",
        "- ```torch.Tensor.view``` -> allows you to reshape a **Tensor** (multi-dimentional vector)"
      ]
    },
    {
      "metadata": {
        "id": "LdVgA9-Ty_lA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        '''\n",
        "        INSERT YOUR MODEL COMPONENTS HERE \n",
        "        '''\n",
        "        return x\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        LINK YOUR COMPONENTS HERE \n",
        "        '''\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YREbJAQRnlIw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "Again we are going to set our training hyper parameters\n",
        "\n",
        "- **epochs** -> The number of times we show the training dataset to the model \n",
        "- **learning rate** -> The size of the step we take each time we go through backpropogation (i.e. the $\\alpha$ in $W_{t+1} = W_t - \\alpha \\nabla_W\\mathcal L$)\n",
        "- **momentum** (_Note: this will only apply if you use SGD_) -> We don't want to get stuck in local minima in the loss landscape so we may want to not let a particularly bad batch prevent otherwise good progress. We can redefine the parameter update procedure as $W_{t+1} = W_t - \\alpha V_t$ and $V_t = \\beta V_{t-1} + (1 - \\beta)\\nabla_W\\mathcal L$ where $\\beta$\n",
        "is our notion of momentum\n",
        "\n",
        "Set these values to something you think might be reasonable and see what happens. Make sure to ```shift+enter``` to lock them in."
      ]
    },
    {
      "metadata": {
        "id": "D9cKjBv79-Ru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Training Hyper Parameters\n",
        "EPOCHS = 10                #@param {type:\"integer\"}           #Number of times to go through the data set\n",
        "SGD_MOMENTUM = 0.5        #@param {type:\"number\"}            #How much it takes to change the direction of the gradient\n",
        "LEARNING_RATE = 0.001     #@param {type:\"number\"}            #How far each update pushes the weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WIIjJECtn6Dk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again create an instance of your model and transfer it to the GPU (using the ```nn.Module.cuda``` method).\n",
        "\n",
        "Now chose your optimizer. Take a look at http://pytorch.org/docs/master/optim.html#algorithms for a list of choices, but if you need a recomendation use ```torch.optim.SGD```\n",
        "\n",
        "Next we need to pick a **loss** function.  Here is a list http://pytorch.org/docs/master/nn.html#id46\n",
        "If you are not sure what to choose then use ```torch.nn.functional.nll_loss``` which is negative log likelihood loss ($L(y) = -log(y)$) a common function to use with softmax"
      ]
    },
    {
      "metadata": {
        "id": "cG7X1IAh0PAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlp_model = MLP()\n",
        "mlp_model.cuda()\n",
        "# Change this to whatever optimizer you want to try\n",
        "mlp_optimizer = optim.SGD(mlp_model.parameters(), lr=LEARNING_RATE, momentum=SGD_MOMENTUM)\n",
        "print(mlp_model)\n",
        "#Change the last argument to whatever loss function you want to try \n",
        "MLP_TRAINING_LOSS, MLP_TEST_LOSS, MLP_TEST_ACCURACY = learn(mlp_model, mlp_optimizer, F.nll_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbLbDMUQpMum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing How Well the Training Went\n",
        "\n",
        "Now we should have a reasonably well trained model. Lets see how the train went. Use the visualization function to plot the **training loss**, **testing loss** and **testing accuracy** over the course of the training. \n",
        "\n",
        "_Notice: We do not measure the training accuracy, since it is not a good measure of how good the model is since we are looking for a model that has good performace **on data not yet seen**_\n",
        "\n",
        "We should see the training loss quickly decline then plateau. \n",
        "  - If you are seeing jagged training loss perhaps increase the batch size or decrease the learning rate \n",
        "  \n",
        "You should see that the testing loss more gradually reduces \n",
        "  - if you see the testing loss starting to go up, you are now overfitting - probably reduce the number of epochs\n",
        "\n",
        "You should also see the testing accuracy increase at the same rate the testing loss decreases"
      ]
    },
    {
      "metadata": {
        "id": "W4zSlFMw4ZhG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_learning(MLP_TRAINING_LOSS, MLP_TEST_LOSS, MLP_TEST_ACCURACY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3qk7rt_pU8G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trying an Example on Your Trained Model\n",
        "\n",
        "Use the ```classify_an_example``` helper function to classify an example from the testing set"
      ]
    },
    {
      "metadata": {
        "id": "nZF8t3AB_-Sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classify_an_example(mlp_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AG4tBhF3uF6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing LeNet\n",
        "\n",
        "Our final model of the day is going to be LeNet (LeCun 1998). LeNet uses convolution layers to learn usuful features in the image instead of relying on just pixels. Then by stacking more convolution layers on top the model begins to extra useful collections of features. This terminates in a MLP which tries to associate this high level representation of the image with the label. \n",
        "\n",
        "Now our model looks like this $\\sigma(w^T relu(w^T relu(w^T relu(w * relu( w * x + b) + b)+ b)+ b) + b)$ and again we can train this with gradient decent"
      ]
    },
    {
      "metadata": {
        "id": "t4RSLeoaoJKe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the Model\n",
        "Once again we are going to create a sub class of the pytorch class ```torch.nn.Module``` listing out the layers of our model in our ```__init__``` method and the operations between those layers in the method called ```forward```. The ```torch.nn.Module``` super class handles the implementation of the backpropogation.\n",
        "\n",
        "Go a head and try to fill in what you think a Multilayer Perceptron Model would look like.\n",
        "\n",
        "Here are some useful functions:\n",
        "\n",
        "- ```torch.nn.Conv2d``` -> 2 Dimentional Convolution / Implements $F(\\boldsymbol ) = \\boldsymbol b + \\sum_{k=0}^{C_{in} - 1} \\boldsymbol w * \\boldsymbol x$\n",
        "- ```torch.nn.Linear``` -> Fully Connected Layer / Impliments $F(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + \\boldsymbol b$\n",
        "- ```torch.nn.Functional.relu``` -> Rectifying Linear Unit / Implements $max(0,x)$ \n",
        "    - Other activation functions can be found here: http://pytorch.org/docs/master/nn.html#non-linear-activation-functions \n",
        "- ```torch.nn.Functional.softmax``` -> Softmax function / Implements $F(x) = \\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} }$\n",
        "- ```torch.nn.Functional.log_softmax``` -> Log of the softmax function / Implements $F(x) = log(\\frac{e^{ \\boldsymbol x}} {\\sum_\\limits{i \\in dim \\boldsymbol x} e^{\\boldsymbol x_i} })$\n",
        "- ```torch.Tensor.view``` -> allows you to reshape a **Tensor** (multi-dimentional vector)\n",
        "- ```torch.nn.Dropout2d``` -> randomly set a percentage of weights to randomly be set to 0 each update to prevent overfitting\n"
      ]
    },
    {
      "metadata": {
        "id": "-6ioVqV2lOw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Network\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        '''\n",
        "        INSERT YOUR MODEL COMPONENTS HERE\n",
        "        ''' \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        LINK YOUR COMPONENTS HERE \n",
        "        '''\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uddr0G8Epm1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "Again we are going to set our training hyper parameters\n",
        "\n",
        "- **epochs** -> The number of times we show the training dataset to the model \n",
        "- **learning rate** -> The size of the step we take each time we go through backpropogation (i.e. the $\\alpha$ in $W_{t+1} = W_t - \\alpha \\nabla_W\\mathcal L$)\n",
        "- **momentum** (_Note: this will only apply if you use SGD_) -> We don't want to get stuck in local minima in the loss landscape so we may want to not let a particularly bad batch prevent otherwise good progress. We can redefine the parameter update procedure as $W_{t+1} = W_t - \\alpha V_t$ and $V_t = \\beta V_{t-1} + (1 - \\beta)\\nabla_W\\mathcal L$ where $\\beta$\n",
        "is our notion of momentum\n",
        "\n",
        "Set these values to something you think might be reasonable and see what happens. Make sure to ```shift+enter``` to lock them in."
      ]
    },
    {
      "metadata": {
        "id": "vTU8GISN-Dzi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Training Hyper Parameters\n",
        "EPOCHS = 5                #@param {type:\"integer\"}           #Number of times to go through the data set\n",
        "SGD_MOMENTUM = 0.5        #@param {type:\"number\"}            #How much it takes to change the direction of the gradient\n",
        "LEARNING_RATE = 0.001     #@param {type:\"number\"}            #How far each update pushes the weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfRkyLxkpr7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again create an instance of your model and transfer it to the GPU (using the ```nn.Module.cuda``` method).\n",
        "\n",
        "Now chose your optimizer. Take a look at http://pytorch.org/docs/master/optim.html#algorithms for a list of choices, but if you need a recomendation use ```torch.optim.SGD```\n",
        "\n",
        "Next we need to pick a **loss** function.  Here is a list http://pytorch.org/docs/master/nn.html#id46\n",
        "If you are not sure what to choose then use ```torch.nn.functional.nll_loss``` which is negative log likelihood loss ($L(y) = -log(y)$) a common function to use with softmax"
      ]
    },
    {
      "metadata": {
        "id": "X4AZCoK2laMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_model = LeNet()\n",
        "cnn_model.cuda()\n",
        "# Change this to whatever optimizer you want to try\n",
        "cnn_model_optimizer = optim.SGD(cnn_model.parameters(), lr=LEARNING_RATE, momentum=SGD_MOMENTUM)\n",
        "print(cnn_model)\n",
        "#Change the last argument to whatever loss function you want to try \n",
        "CNN_TRAINING_LOSS, CNN_TEST_LOSS, CNN_TEST_ACCURACY = learn(cnn_model, cnn_model_optimizer, F.nll_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RINtmxBzpa-i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing How Well the Training Went\n",
        "\n",
        "Now we should have a reasonably well trained model. Lets see how the train went. Use the visualization function to plot the **training loss**, **testing loss** and **testing accuracy** over the course of the training. \n",
        "\n",
        "_Notice: We do not measure the training accuracy, since it is not a good measure of how good the model is since we are looking for a model that has good performace **on data not yet seen**_\n",
        "\n",
        "We should see the training loss quickly decline then plateau. \n",
        "  - If you are seeing jagged training loss perhaps increase the batch size or decrease the learning rate \n",
        "  \n",
        "You should see that the testing loss more gradually reduces \n",
        "  - if you see the testing loss starting to go up, you are now overfitting - probably reduce the number of epochs\n",
        "\n",
        "You should also see the testing accuracy increase at the same rate the testing loss decreases"
      ]
    },
    {
      "metadata": {
        "id": "_FtgkF-zl8wW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_learning(CNN_TRAINING_LOSS, CNN_TEST_LOSS, CNN_TEST_ACCURACY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8n9258aCpY8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trying an Example on Your Trained Model\n",
        "\n",
        "Use the ```classify_an_example``` helper function to classify an example from the testing set"
      ]
    },
    {
      "metadata": {
        "id": "weQjoVft57_y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classify_an_example(cnn_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UflW59X1SOn0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Congratulations!\n",
        "\n",
        "You just trained 3 different machine learning models and all of them are relatively good at detecting handwritting. There is a big world of models, tasks and strategies to explore. But one more thing before we finish this tutorial..."
      ]
    },
    {
      "metadata": {
        "id": "fk5GUPekSwAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Limits of your model\n",
        "\n",
        "You trained a model, it gets something like 98% accuracy on your testing dataset. But is it ready to go out into the world and classify digits?\n",
        "\n",
        "Lets take a look at an example again... "
      ]
    },
    {
      "metadata": {
        "id": "9O_oxxdHTKQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classify_an_example(cnn_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plHixLqrTNLe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But what if I give you this image? "
      ]
    },
    {
      "metadata": {
        "id": "AnTHDEUiWV76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img, label = next(iter(test_loader))\n",
        "img = img[0][0]\n",
        "label = label[0]\n",
        "img_transpose = img.transpose(0,1)\n",
        "img_view = img_transpose.cpu().numpy()\n",
        "plt.imshow(img_view.reshape(28,28))\n",
        "print(\"It is supposed to be a \" + str(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VToGFkWNXVCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"The model thinks it is a \" + str(classify(cnn_model, img_transpose.unsqueeze(0).unsqueeze(0))[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pSJz8SmZRXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What could we do to solve this problem?\n",
        "\n",
        "This is the main job of ML engineers..."
      ]
    }
  ]
}
